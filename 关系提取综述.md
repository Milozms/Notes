# 关系提取综述

整理与翻译：张茂森

## 摘要
本文内容整理并翻译自Nguyen Bach与Sameer Badaskar所写的A Review of Relation Extraction一文，与Shantanu Kumar所写的A Survey of Deep Learning Methods for Relation Extraction一文。

## 导论
关系提取是指从无结构文本中提取实体之间的关系，从而将其转化为结构化信息的过程。关系定义在实体的元组$ t = (e_1, e_2, ..., e_n) $ 之上。大多数关系提取系统着重于提取二元关系，例如*坐落于（卡耐基梅隆大学，匹兹堡）*，*父亲（Manuel Blum, Avrim Blum）*。高阶关系与之类似。

有监督方法：将关系提取问题公式化为二分类问题：

- 基于特征的监督方法(Kambhatla, 2004)(Zhao & Grishman, 2005)
- 基于核的监督方法：能够高效地在多项式时间内搜索足够大的特征空间（通常是指数级的，在某些例子中是无穷的），并且不需要显式地表示特征。
  - 树形核、子序列核、依存树核。

半监督方法和bootstrapping方法：

- DIPRE(Brin, 1998)和Snowball(Agichtein & Gravano, 2000)，这类方法只需要少量的人工标记的种子实例或少量的人工提取模式来启动训练，使用了类似于Yarowsky的词语消歧义算法的半监督方法；
- KnowItAll(Etzioni, 2005)和TextRunner(Banko, 2007)提出了使用自训练关系二分类器的大规模关系提取系统。

## 有监督方法
出于简便，我们仅讨论两个实体之间的二元关系。多元关系在之后讨论。

给定一个句子$S = w_1, w_2, ..., e_1, ..., w_j, ..., e_2, ..., w_n$，其中$e_1, e_2$是实体，我们给出一个映射函数$f$：
$$
f_R(T(S)) = 
\left\{  
             \begin{array}{lr}  
             +1, & If\ e_1\ and\ e_2\ are\ related\ according\ to\ relation\ R \\
             -1, & otherwise   
             \end{array}  
\right.  
$$

其中$T(S)$是从S中提取的特征。映射函数$f$为+1表示实体之间存在关系，$f$为-1表示实体之间不存在关系。如果标记的正样例和负样例数据可以用来训练，则函数$f$可以被构建成一个判别分类器（例如感知器、支持向量机等），经过预处理（词性标注、依存分析）之后用文本级特征来训练。分类器的输入也可以是结构化表示，如解析树。由于分类器训练的输入的不同，监督方法也可以分为基于特征的方法和核方法。

### 基于特征的方法
从文本中提取的句法和语义的特征，可以作为判定句子中的实体是否有某种关系的线索。

- 句法特征：实体本身，实体类别，实体之间的词序列，实体之间的词的数量，解析树中实体之间的路径。
- 语义特征：依存分析树中实体之间的路径。

这些特征都可以在特征向量中表示并输入到分类器中。

自然语言处理的应用往往涉及输入数据的结构化表示，因此仅使用相关特征的子集很难达到最优。作为补充，人们设计了专用于关系提取的核，用于更充分地利用输入数据的表示。

### 核方法
关系提取的核方法基于字符串核（string-kernels），这一方法源于文本分类。给出字符串x和y，字符串核基于它们的公共子串数量计算它们的相似度。每个字符串被映射到高维空间中的向量，每个维度对应一个特定的子串是否出现。例如字符串cat可做如下表示：

$$
\phi(x=cat) = [\phi_a(x)\ ..\ \phi_c(x)\ ..\ \phi_t(x)\ ..\ \phi_{at}(x)\ ..\ \phi_{ca}(x)\ ..\ \phi_{ct}(x)\ ..\ \phi_{cat}(x)] 
$$
其中

$$
\phi_a(x)=\phi_c(x)=\phi_t(x) = \lambda \\
\phi_{at}(x)=\phi_{ca}(x)=\phi_{ct}(x) = \lambda^2\\
\phi_{cat}(x) = \lambda^3
$$
$\lambda$被称为衰减因子，用于惩罚较长的、不相邻的子序列。事实上$\phi_{ct}(cat)$应该比$\phi_{at}(cat)$和$\phi_{ca}(cat)$受到更多惩罚（$\lambda^3$），因为不相邻。对于索引为$i = i_1, i_2, ..., i_{|u|}$的子串$u$，令$u$的长度为$l(i)=i_{|u|}-i_1+1$，$u$在字符串x的高维空间中对应的坐标值为

$$
\phi_u(x) = \sum_{i:u=x[i]} \lambda^{l(i)}
$$
（我的理解：这里的求和的意思是，如果子序列u在x中出现多次，则把多次的衰减因子相加。上述例子中ct的衰减因子应为3次方。）

若U是字符串x和y包括的所有可能的子串，则x和y的核相似度为
$$
K(x,y)=\phi(x)^T \phi(y) = \sum_{u \in U}\phi_u(x)^T \phi_u(y)
$$
这一相似度的计算可以使用动态规划，复杂度为$O(|x|||y|^2)$。

如果x和y是两个对象，$K(x,y)$可以理解为：通过计算他们之间结构的共同点，从而计算他们的相似度。

在关系提取中，如果$x^+$和$x^-$分别表示训练数据中的正样例和负样例，而y表示测试样例，则$K(x^+,y)>K(x^-,y)$意味着y包含一个关系（y更接近正样例）。实际上$K(x,y)$就是支持向量机和感知机中使用的相似性函数。在关系提取中，$x^+$、$x^-$和y可以用实体之间的词序列和包含实体的解析树表示。

#### 特征包核(Bag of features)
以句子"*The headquarters of* **Google** *are situated in* **Mountain View**"为例。已知Google和Mountain View是实体，词situated和headquarters表明*机构-位置*关系，则我们可以得出：实体的上下文可以用来判定它们之间是否有关系。句子s中包含实体$e_1$和$e_2$，可以被表示为$s = sb\ e_1\ sm\ e_2\ sa$，其中sb、sm、sa分别表示在实体之前、之间、之后的上下文词。给定测试样例句子t，t包含实体实体$e'_1$和$e'_2$，使用子序列和计算两个句子的三组上下文之间的相似度（这里使用词语级序列，而不是字符级序列），得到三个子核，三个子核结合就得到了最终的核。用子序列核结合支持向量机，显著提高了关系提取的准确率。

#### 树形核

树形核用于计算两个entity-augmented shallow parse tree结构之间的相似度。（之所以使用shallow parse tree而不用完全的解析树，是因为shallow parse tree的鲁棒性和稳定性更好。）Shallow parser用实体信息和短语信息来使树增强。给定一个shallow parse：

- 正样例：枚举最低的覆盖相关的实体的子树；

- 负样例：如果子树不覆盖两个相关的实体。

样例树中的每个节点有三个属性：
- entity role(人，机构，非实体) 

- chunk type(如NP, VP)

- 节点覆盖的文本

树形核的核函数是在字符串核的基础上改进的，计算两个shallow parse tree之间的结构共同点：计算两个shallow parse tree的公共子树个数的加权和。如下递归计算：
给定两个子树，根节点分别为$T_1$和$T_2$：
- 1、比较$T_1$和$T_2$的三种属性，如果不匹配，则返回相似度0；
- 2、如果三种属性匹配，则在总分上加1，并比较$T_1$和$T_2$的子节点序列。记$children(T_1)$和$children(T_2)$分别为$T_1$和$T_2$的子节点序列，他们的相似度由公共子节点子序列的数量计算，计算的方法与字符串核相同。

如果m和n是shallow parse tree的节点数，则核的计算复杂度为$O(mn^3)$。

而2005年Bunescu和Mooney观察到：依存树中两个实体之间的最短路径已经包含了充足的信息：若$e_1$和$e_2$是句子中的两个实体而p是它们之间的谓词，则$e_1$和$e_2$之间的最短路径经过p，因为$e_1$和$e_2$是p的参数。
